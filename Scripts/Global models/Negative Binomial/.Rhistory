{
stationarity.plot(alpha[,jj],xlab="iteration",ylab=paste(expression(alpha),jj))
}
beta = as.data.frame(data.out[,grepl("beta", names(data.out))])
x11()
par(mfrow=c(1,dim(beta)[2]),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
for (jj in 1:dim(beta)[2])
{
stationarity.plot(beta[,jj],xlab="iteration",ylab=paste(expression(beta),jj))
}
lambda = as.data.frame(data.out[,grepl("lambda", names(data.out))])
lambda.plot = lambda[,c(1,5,10,15,20,25,30,35)]
x11()
par(mfrow=c(1,dim(lambda.plot)[2]),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
for (jj in 1:dim(lambda.plot)[2])
{
stationarity.plot(lambda.plot[,jj],xlab="iteration",ylab=paste(expression(lambda),jj))
}
#### 1 - check if beta parameters are significant, so if they are different from 0
beta_quantile <- apply(beta,2,quantile,c(0.05,0.95))
beta_m <- apply(beta,2,mean)
beta_quantile
x11()
par(mfrow=c(3,3))
hist(alpha[,], nclass="fd", main="Intercept", xlab=expression(alpha), prob=T)
lines(density(alpha[,]),col="blue",lwd=2)
quantile(alpha[,],prob=c(0.025,0.5,0.975))
abline(v=quantile(alpha[,],prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(alpha[,],prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(alpha[,],prob=c(0.975)),col="red",lty=2,lwd=2)
for (i in 1:7){
hist(beta[,i], nclass="fd", main=colnames(volume)[i+1], xlab=paste(expression(beta),i), prob=T)
lines(density(beta[,i]),col="blue",lwd=2)
quantile(beta[,i],prob=c(0.025,0.5,0.975))
abline(v=quantile(beta[,i],prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.975)),col="red",lty=2,lwd=2)
}
x11()
par(mfrow=c(3,3))
hist(alpha[,], nclass="fd", main="Intercept", xlab=expression(alpha), prob=T)
lines(density(alpha[,]),col="blue",lwd=2)
quantile(alpha[,],prob=c(0.025,0.5,0.975))
abline(v=quantile(alpha[,],prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(alpha[,],prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(alpha[,],prob=c(0.975)),col="red",lty=2,lwd=2)
for (i in 1:7){
hist(beta[,i], nclass="fd", main=colnames(volume)[i+1], xlab=paste(expression(beta),i), prob=T)
lines(density(beta[,i]),col="blue",lwd=2)
quantile(beta[,i],prob=c(0.025,0.5,0.975))
abline(v=quantile(beta[,i],prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.975)),col="red",lty=2,lwd=2)
}
#### 3 - some plot of the posterior distributions of y(t)s (for t = 1, 2 for example)
pred_y <- as.data.frame(data.out[,grepl("Ypred", names(data.out))])
pred_int <- apply(pred_y,2,quantile,c(.05,.95))
pred_mean <- apply(pred_y,2,mean)
pred_sd <- apply(pred_y,2,sd)
## Plot of distribution of Y with posterior parameters, comparison with real Y - 90% credibility interval
sz = dim(data.out)[1]
x11()
plot(1:35, pred_mean, pch = 16, col = 'red', type ='o', xlab = 'day', ylab='traffic', main ='comparison between actual and predicted Y')
points(1:35, volume$Y, pch=15)
for(ii in 1:35)
{
points(ii, quantile(pred_y[,ii], 0.05), col = 'darkred', pch = 16)
points(ii, quantile(pred_y[,ii], 0.95), col = 'darkred', pch = 16)
segments(ii, quantile(pred_y[,ii] , 0.05), ii, quantile(pred_y[,ii], 0.95), col  = 'darkred')
if(ii!=35)
{
lines(c(ii, ii+1), c(quantile(pred_y[,ii] , 0.05), quantile(pred_y[,ii+1], 0.05)), col = 'darkred', lty = 3)
lines(c(ii, ii+1), c(quantile(pred_y[,ii] , 0.95), quantile(pred_y[,ii+1], 0.95)), col = 'darkred', lty = 3)
}
}
rm(list=ls())
library(coda)
library(plotrix)
library(loo)
library(dplyr)
library(rjags)
# load dataset and weather, for computing the design matrix X
load("../../../Dataset/bikemi_data.RData")
data <- bike_nil_week_hour
weather_data <- read.table("../../../Dataset/data_weather.csv", header = T, sep = ",")
total_traffic <- data %>% group_by(NewDay) %>% summarize(n())
names(total_traffic) <- c("Day","N_travels")
setwd("~/Documents/V anno/Bayesian/BayesProject/Bayesian_project/Scripts/Global models/Negative Binomial")
Rt_class = read.csv('weat.csv')[,1]
Y <- as.matrix(total_traffic)[1:42,2]
volume <- as.data.frame(cbind(Y,rep(c(rep(0,5),rep(1,2)),6), Rt_class ,weather_data$Tmean))
colnames(volume) <- c("Y","W","Rt_class","T")
## Define the data
dat = list(Y=volume$Y, X=cbind(1,volume[,2:4]), n=42, mu.beta=rep(0,4), tau.beta=diag(rep(0.001,4)))
## A list of initial value for the MCMC algorithm in JAGS
inits = function() {list(beta=rep(0,4))}
## Define model
modelRegress=jags.model("NegBin.bug", data=dat, inits=inits,
n.adapt=100000, n.chains=3)
## Save model
save(modelRegress,file="NegBin_model.Rdata")
## Delete burnin
update(modelRegress, n.iter=200000) # this is the burn-in period
## tell JAGS to generate MCMC samples that we will use
## to represent the posterior distribution
variable.names <- c("beta", "Ypred", "p_param", "r_param", "lambda")
# their values recorded
n.iter=300000
thin=100
outputRegress = coda.samples(model=modelRegress, variable.names=variable.names,
n.iter=n.iter, thin=thin)
## Save the model
save(outputRegress,file= 'NegBin_W_Rclass_T_output.Rdata')
##---------------------------------------------##
#### Output analyisis
library(coda)       # per leggere catena mcmc
library(plotrix)    # per fare plot CIs
## Load trajectories
## the OUTPUT is mcmc.list object - coda-formatted object;
## it needs to be converted into a matrix, in order to be "readable".
data.out=as.matrix(outputRegress) # trasform the mcmc.list into a matrix,
data.out=data.frame(data.out)     # or, better, into a dataframe (easiest to handle in R)
attach(data.out)
n.chain=dim(data.out)[1]
n.chain # this is the final sample size
summary(data.out)
head(data.out)
## Validation of the stationarity
stationarity.plot<-function(x,...){
S<-length(x)
scan<-1:S
ng<-min( round(S/100),10)
group<-S*ceiling( ng*scan/S) /ng
boxplot(x~group,...)
}
beta = as.data.frame(data.out[,grepl("beta", names(data.out))])
x11()
par(mfrow=c(1,dim(beta)[2]),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
for (jj in 1:dim(beta)[2])
{
stationarity.plot(beta[,jj],xlab="iteration",ylab=paste(expression(beta),jj))
}
r_param = as.data.frame(data.out[,grepl("r_param", names(data.out))])
x11()
par(mfrow=c(1,dim(r_param)[2]),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
for (jj in 1:dim(r_param)[2])
{
stationarity.plot(r_param[,jj],xlab="iteration",ylab=paste(expression(r),jj))
}
p_param = as.data.frame(data.out[,grepl("p_param", names(data.out))])
p_param.plot = p_param[,c(1,5,10,15,20,25,30,35)]
x11()
par(mfrow=c(1,dim(p_param.plot)[2]),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
for (jj in 1:dim(p_param.plot)[2])
{
stationarity.plot(p_param.plot[,jj],xlab="iteration",ylab=paste(expression(p),jj))
}
lambda = as.data.frame(data.out[,grepl("lambda", names(data.out))])
lambda.plot = lambda[,c(1,5,10,15,20,25,30,35)]
x11()
par(mfrow=c(1,dim(lambda.plot)[2]),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
for (jj in 1:dim(lambda.plot)[2])
{
stationarity.plot(lambda.plot[,jj],xlab="iteration",ylab=paste(expression(lambda),jj))
}
## Autocorrelation beta
x11()
par(mfrow=c(1,dim(beta)[2]))
for(i in 1:dim(beta)[2]){
acf(beta[,i],lwd=3,col="red3", main=paste(expression(beta),i))
}
## Autocorrelation r
x11()
par(mfrow=c(1,dim(r_param)[2]))
for(i in 1:dim(r_param)[2]){
acf(r_param[,i],lwd=3,col="red3", main=expression(r))
}
#### 1 - check if beta parameters are significant, so if they are different from 0
beta_quantile <- apply(beta,2,quantile,c(0.05,0.95))
beta_m <- apply(beta,2,mean)
beta_quantile
# Plot for beta
nam <- colnames(volume)
nam[1] <- "Intercept"
x11()
par(mfrow=c(2,3))
for (i in 1:dim(beta)[2]){
hist(beta[,i], nclass="fd", main=nam[i], xlab=paste(expression(beta),i), prob=T)
lines(density(beta[,i]),col="blue",lwd=2)
quantile(beta[,i],prob=c(0.025,0.5,0.975))
abline(v=quantile(beta[,i],prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.975)),col="red",lty=2,lwd=2)
legend("topright",legend=c("posterior median", "95% Credible bounds","kernel density smoother"),lwd=c(2,2,2), col=c("red","red","blue"),lty=c(1,2,1))
}
x11()
par(mfrow=c(2,2))
for (i in 1:dim(beta)[2]){
hist(beta[,i], nclass="fd", main=nam[i], xlab=paste(expression(beta),i), prob=T)
lines(density(beta[,i]),col="blue",lwd=2)
quantile(beta[,i],prob=c(0.025,0.5,0.975))
abline(v=quantile(beta[,i],prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.975)),col="red",lty=2,lwd=2)
legend("topright",legend=c("posterior median", "95% Credible bounds","kernel density smoother"),lwd=c(2,2,2), col=c("red","red","blue"),lty=c(1,2,1))
}
x11()
par(mfrow=c(2,2))
for (i in 1:dim(beta)[2]){
hist(beta[,i], nclass="fd", main=nam[i], xlab=paste(expression(beta),i), prob=T)
lines(density(beta[,i]),col="blue",lwd=2)
quantile(beta[,i],prob=c(0.025,0.5,0.975))
abline(v=quantile(beta[,i],prob=c(0.025)),col="red",lty=2,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.5)),col="red",lty=1,lwd=2)
abline(v=quantile(beta[,i],prob=c(0.975)),col="red",lty=2,lwd=2)
}
#### 3 - some plot of the posterior distributions of y(t)s (for t = 1, 2 for example)
pred_y <- as.data.frame(data.out[,grepl("Ypred", names(data.out))])
pred_int <- apply(pred_y,2,quantile,c(.05,.95))
pred_mean <- apply(pred_y,2,mean)
pred_sd <- apply(pred_y,2,sd)
## Plot of distribution of Y with posterior parameters, comparison with real Y - 90% credibility interval
sz = dim(data.out)[1]
x11()
plot(1:42, pred_mean, pch = 16, ylim=c(0,30000), col = 'red', type ='o', xlab = 'day', ylab='traffic', main ='comparison between actual and predicted Y')
points(1:42, volume$Y, pch=15)
for(ii in 1:42)
{
points(ii, quantile(pred_y[,ii], 0.05), col = 'darkred', pch = 16)
points(ii, quantile(pred_y[,ii], 0.95), col = 'darkred', pch = 16)
segments(ii, quantile(pred_y[,ii] , 0.05), ii, quantile(pred_y[,ii], 0.95), col  = 'darkred')
if(ii!=42)
{
lines(c(ii, ii+1), c(quantile(pred_y[,ii] , 0.05), quantile(pred_y[,ii+1], 0.05)), col = 'darkred', lty = 3)
lines(c(ii, ii+1), c(quantile(pred_y[,ii] , 0.95), quantile(pred_y[,ii+1], 0.95)), col = 'darkred', lty = 3)
}
}
####### 3 - PREDICTIVE GOODNESS-OF-FIT
N_obs <- length(volume$Y)
# A - OUTLIERS
outlier=(volume$Y>pred_int[2,]| volume$Y<pred_int[1,])
sum(outlier) # number of obs falling outside the credible intervals: 2 out of 35
# B - Bayesian predictive p-value
ind_pvalue=t((t(pred_y) > volume$Y))
pred_suptail=apply(ind_pvalue,2,sum)/n.chain
prob_tail<-rep(0, N_obs)
for (i in 1 : N_obs){
prob_tail[i]=min(pred_suptail[i],1-pred_suptail[i])
}
x11()
plot(ind, prob_tail)
abline(h=0.05)
ind=1:42
x11()
plot(ind, prob_tail)
abline(h=0.05)
# OUTLIERS: those obs such that the BAYESIAN PREDICTIVE p-value
#   is smaller than 0.05 (or than 0.1)
out2=(prob_tail<0.05)
sum(out2) # the same as before: 2 out of 35 outliers
bres <- (volume$Y - pred_mean)/pred_sd
out_res = (abs(bres) > 2) #as a reference value we take 2
x11()
par(mfrow=c(1,1))
plot(ind,bres,cex=1)
abline(h=c(-2,2), lty = "dashed")
x11()
par(mfrow=c(1,dim(beta)[2]+1))
for(i in 1:dim(beta)[2]){
acf(beta[,i],lwd=3,col="red3", main=paste(expression(beta),i))
}
for(i in 1:dim(r_param)[2]){
acf(r_param[,i],lwd=3,col="red3", main=expression(r))
}
x11()
par(mfrow=c(1,dim(beta)[2]+1))
for(i in 1:dim(beta)[2]){
acf(beta[,i],lwd=3,col="red3", main=paste(expression(beta),i))
}
for(i in 1:dim(r_param)[2]){
acf(r_param[,i],lwd=3,col="red3", main=expression(r))
}
x11()
par(mfrow=c(1,dim(beta)[2]+1))
for(i in 1:dim(beta)[2]){
acf(beta[,i],lwd=3,col="red3", main=paste(expression(beta),i))
}
for(i in 1:dim(r_param)[2]){
acf(r_param[,i],lwd=3,col="red3", main=paste(expression(r)))
}
r_param = as.data.frame(data.out[,grepl("r_param", names(data.out))])
x11()
par(mfrow=c(1,dim(r_param)[2]),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
for (jj in 1:dim(r_param)[2])
{
stationarity.plot(r_param[,jj],xlab="iteration",ylab=paste(expression(r),jj))
}
beta = as.data.frame(data.out[,grepl("beta", names(data.out))])
x11()
par(mfrow=c(1,dim(beta)[2]),mar=c(2.75,2.75,.5,.5),mgp=c(1.7,.7,0))
for (jj in 1:dim(beta)[2])
{
stationarity.plot(beta[,jj],xlab="iteration",ylab=paste(expression(beta),jj))
}
load("NegBin_var_est.dat")
rstan::traceplot(NEGBIN_GLMM, pars = "sigma2_beta", inc_warmup = TRUE)
rstan::traceplot(NEGBIN_GLMM, pars= "sigma2_theta", inc_warmup = TRUE)
rstan::traceplot(NEGBIN_GLMM, pars = "beta", inc_warmup = FALSE)
rstan::traceplot(NEGBIN_GLMM, pars= "theta", inc_warmup = FALSE)
rstan::traceplot(NEGBIN_GLMM, pars = "beta", inc_warmup = FALSE)
rstan::traceplot(NEGBIN_GLMM, pars= "theta", inc_warmup = FALSE)
coda_chain <- As.mcmc.list(NEGBIN_GLMM, pars = c("beta", "theta"))
summary(coda_chain)
# beta
plot_post <- NEGBIN_GLMM %>%
rstan::extract("beta") %>%
as.data.frame() %>%
map_df(as_data_frame, .id = 'param')
plot_post %>%
ggplot(aes(value, fill = param)) +
geom_density() +
facet_wrap(~param, scales = 'free') +
scale_fill_locuszoom() +
theme_minimal() +
theme(legend.position="none")
# beta
plot_post <- NEGBIN_GLMM %>%
rstan::extract("beta") %>%
as.data.frame() %>%
map_df(as_data_frame, .id = 'param')
plot_post %>%
ggplot(aes(value, fill = param)) +
geom_density() +
facet_wrap(~param, scales = 'free') +
scale_fill_locuszoom() +
theme(legend.position="none")
# beta
plot_post <- NEGBIN_GLMM %>%
rstan::extract("beta") %>%
as.data.frame() %>%
map_df(as_data_frame, .id = 'param')
plot_post %>%
ggplot(aes(value, fill = param)) +
geom_density() +
facet_wrap(~param, scales = 'free') +
scale_fill_locuszoom() +
theme_minimal()
# beta
plot_post <- NEGBIN_GLMM %>%
rstan::extract("beta") %>%
as.data.frame() %>%
map_df(as_data_frame, .id = 'param')
plot_post %>%
ggplot(aes(value, fill = param)) +
geom_density() +
facet_wrap(~param, scales = 'free') +
scale_fill_locuszoom() +
theme_minimal()+
theme(legend.position="none")
# theta
plot_post <- NEGBIN_GLMM %>%
rstan::extract("theta") %>%
as.data.frame() %>%
map_df(as_data_frame, .id = 'param')
plot_post %>%
ggplot(aes(value, fill = param)) +
geom_density() +
facet_wrap(~param, scales = 'free') +
scale_fill_locuszoom() +
theme_minimal() +
coord_flip() +
theme(legend.position="none")
params <- rstan::extract(NEGBIN_GLMM, c("beta", "theta", "r_param"), perm = T)
jpar <- cbind(params[[1]], params[[2]], params[[3]])
beta.data <- jpar[,c(1,2,3)]
theta.data <- jpar[,c(4,5)]
r_param.data <- jpar[,c(6,7)]
pred_y <- matrix(0,nrow=nrow(jpar),ncol = 42)
for(i in 1:42){
for(j in 1:nrow(jpar)){
mu <- exp(sum(beta.data[j,]*X[i,]) + sum(theta.data[j,]*G[i,]))
r <- sum(r_param.data[j,]*G[i,])
p <- r/(r+mu)
pred_y[j,i] <- rnbinom(1,size = r, prob = p)
}
}
# load dataset and weather, for computing the design matrix X
load("../../../Dataset/bikemi_data.RData")
data <- bike_nil_week_hour
weather_data <- read.table("../../../Dataset/data_weather.csv", header = T, sep = ",")
total_traffic <- data %>% group_by(NewDay) %>% summarize(n())
names(total_traffic) <- c("Day","N_travels")
Y <- total_traffic$N_travels
X <- as.data.frame(cbind(rep(1,42), weather_data$rain, weather_data$Tmean))
G <- as.data.frame(cbind(rep(c(rep(1,5),0,0),6), rep(c(rep(0,5),1,1),6)))
colnames(G) <- c("WD","WE")
N <- length(Y)
p_fix <- 3
ngr <- 2
for(i in 1:42){
for(j in 1:nrow(jpar)){
mu <- exp(sum(beta.data[j,]*X[i,]) + sum(theta.data[j,]*G[i,]))
r <- sum(r_param.data[j,]*G[i,])
p <- r/(r+mu)
pred_y[j,i] <- rnbinom(1,size = r, prob = p)
}
}
pred_y
pred_mean <- apply(pred_y,2,mean)
## Plot of distribution of Y with posterior parameters, comparison with real Y - 90% credibility interval
x11()
plot(1:42, pred_mean, pch = 16, ylim=c(0,20000), col = 'red', type ='o', xlab = 'day', ylab='traffic', main ='comparison between actual and predicted Y')
points(1:42, Y, pch=15)
for(ii in 1:42)
{
points(ii, quantile(pred_y[,ii], 0.05), col = 'darkred', pch = 16)
points(ii, quantile(pred_y[,ii], 0.95), col = 'darkred', pch = 16)
segments(ii, quantile(pred_y[,ii] , 0.05), ii, quantile(pred_y[,ii], 0.95), col  = 'darkred')
if(ii!=42)
{
lines(c(ii, ii+1), c(quantile(pred_y[,ii] , 0.05), quantile(pred_y[,ii+1], 0.05)), col = 'darkred', lty = 3)
lines(c(ii, ii+1), c(quantile(pred_y[,ii] , 0.95), quantile(pred_y[,ii+1], 0.95)), col = 'darkred', lty = 3)
}
}
####### 3 - PREDICTIVE GOODNESS-OF-FIT
N_obs <- length(volume$Y)
####### 3 - PREDICTIVE GOODNESS-OF-FIT
N_obs <- length(Y)
# A - OUTLIERS
outlier=(Y>pred_int[2,]| Y<pred_int[1,])
sum(outlier) # number of obs falling outside the credible intervals: 2 out of 35
# B - Bayesian predictive p-value
ind_pvalue=t((t(pred_y) > volume$Y))
pred_suptail=apply(ind_pvalue,2,sum)/n.chain
prob_tail<-rep(0, N_obs)
for (i in 1 : N_obs){
prob_tail[i]=min(pred_suptail[i],1-pred_suptail[i])
}
ind=1:42
x11()
plot(ind, prob_tail)
abline(h=0.05)
# OUTLIERS: those obs such that the BAYESIAN PREDICTIVE p-value
#   is smaller than 0.05 (or than 0.1)
out2=(prob_tail<0.05)
sum(out2) # the same as before: 2 out of 35 outliers
# B - Bayesian predictive p-value
ind_pvalue=t((t(pred_y) > Y))
pred_suptail=apply(ind_pvalue,2,sum)/n.chain
prob_tail<-rep(0, N_obs)
for (i in 1 : N_obs){
prob_tail[i]=min(pred_suptail[i],1-pred_suptail[i])
}
ind=1:42
x11()
plot(ind, prob_tail)
abline(h=0.05)
# OUTLIERS: those obs such that the BAYESIAN PREDICTIVE p-value
#   is smaller than 0.05 (or than 0.1)
out2=(prob_tail<0.05)
sum(out2) # the same as before: 2 out of 35 outliers
bres <- (volume$Y - pred_mean)/pred_sd
bres <- (Y - pred_mean)/pred_sd
out_res = (abs(bres) > 2) #as a reference value we take 2
x11()
par(mfrow=c(1,1))
plot(ind,bres,cex=1)
abline(h=c(-2,2), lty = "dashed")
pred_int <- apply(pred_y,2,quantile,c(.05,.95))
pred_mean <- apply(pred_y,2,mean)
pred_sd <- apply(pred_y,2,sd)
## Plot of distribution of Y with posterior parameters, comparison with real Y - 90% credibility interval
x11()
plot(1:42, pred_mean, pch = 16, ylim=c(0,20000), col = 'red', type ='o', xlab = 'day', ylab='traffic', main ='comparison between actual and predicted Y')
points(1:42, Y, pch=15)
for(ii in 1:42)
{
points(ii, quantile(pred_y[,ii], 0.05), col = 'darkred', pch = 16)
points(ii, quantile(pred_y[,ii], 0.95), col = 'darkred', pch = 16)
segments(ii, quantile(pred_y[,ii] , 0.05), ii, quantile(pred_y[,ii], 0.95), col  = 'darkred')
if(ii!=42)
{
lines(c(ii, ii+1), c(quantile(pred_y[,ii] , 0.05), quantile(pred_y[,ii+1], 0.05)), col = 'darkred', lty = 3)
lines(c(ii, ii+1), c(quantile(pred_y[,ii] , 0.95), quantile(pred_y[,ii+1], 0.95)), col = 'darkred', lty = 3)
}
}
####### 3 - PREDICTIVE GOODNESS-OF-FIT
N_obs <- length(Y)
# A - OUTLIERS
outlier=(Y>pred_int[2,]| Y<pred_int[1,])
sum(outlier) # number of obs falling outside the credible intervals: 4 out of 35
# B - Bayesian predictive p-value
ind_pvalue=t((t(pred_y) > Y))
pred_suptail=apply(ind_pvalue,2,sum)/n.chain
prob_tail<-rep(0, N_obs)
for (i in 1 : N_obs){
prob_tail[i]=min(pred_suptail[i],1-pred_suptail[i])
}
ind=1:42
x11()
plot(ind, prob_tail)
abline(h=0.05)
# OUTLIERS: those obs such that the BAYESIAN PREDICTIVE p-value
#   is smaller than 0.05 (or than 0.1)
out2=(prob_tail<0.05)
sum(out2) # the same as before: 2 out of 35 outliers
bres <- (Y - pred_mean)/pred_sd
out_res = (abs(bres) > 2) #as a reference value we take 2
x11()
par(mfrow=c(1,1))
plot(ind,bres,cex=1)
abline(h=c(-2,2), lty = "dashed")
